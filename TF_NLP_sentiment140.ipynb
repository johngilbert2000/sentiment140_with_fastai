{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was the result of training several models on Sentiment140 using Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version 10.0.130\n",
      "/bin/sh: 1: nvcc: not found\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA Version\n",
    "!cat /usr/local/cuda/version.txt;\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat /usr/local/cuda/include/cudnn.h # | grep CUDNN_MAJOR -A 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.4\n",
      "tensorflow==2.0.0\n",
      "tensorflow-datasets==1.2.0\n",
      "tensorflow-estimator==2.0.0\n",
      "tensorflow-gpu==2.0.0\n",
      "tensorflow-hub==0.6.0\n",
      "tensorflow-metadata==0.15.0\n",
      "pandas==0.25.1\n",
      "numpy==1.17.2\n"
     ]
    }
   ],
   "source": [
    "# Check your versions\n",
    "!python --version;\n",
    "!pip freeze | grep tensorflow;\n",
    "!pip freeze | grep cudf;\n",
    "!pip freeze | grep pandas;\n",
    "!pip freeze | grep numpy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import typing\n",
    "import numbers\n",
    "import os\n",
    "import unittest\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "# Ensure training on one GPU\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PARAMETERS\n",
    "\n",
    "splits = 5 # 5 splits means each validation set is 20% of original set, with 80:20 validation:training set ratio\n",
    "\n",
    "file = 'training.1600000.processed.noemoticon.csv'\n",
    "path = Path('../mydata')\n",
    "\n",
    "df = pd.read_csv(path/file, usecols=[0,5],header=None, names=['label','text'],encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>486678</td>\n",
       "      <td>0</td>\n",
       "      <td>Sleepy as hale. I hope the afternoon goes by s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911387</td>\n",
       "      <td>1</td>\n",
       "      <td>@Hstreet96 always good to see you - even in cy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140877</td>\n",
       "      <td>1</td>\n",
       "      <td>http://tinyurl.com/lsee25 &amp;lt;&amp;lt; my new hams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458442</td>\n",
       "      <td>0</td>\n",
       "      <td>@sgBEAT:MattBinks why not?! You don want me an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1593831</td>\n",
       "      <td>1</td>\n",
       "      <td>on my way to my BNI meeting. should be a good ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "486678       0  Sleepy as hale. I hope the afternoon goes by s...\n",
       "911387       1  @Hstreet96 always good to see you - even in cy...\n",
       "1140877      1  http://tinyurl.com/lsee25 &lt;&lt; my new hams...\n",
       "458442       0  @sgBEAT:MattBinks why not?! You don want me an...\n",
       "1593831      1  on my way to my BNI meeting. should be a good ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPARE DATA\n",
    "\n",
    "# Randomize data\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "# For Binary Classification: Convert Labels to 0 and 1 \n",
    "df.loc[df['label'] == 4, 'label'] = 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SUBSETS\n",
    "\n",
    "# Equal length subsets of original dataframe\n",
    "cut_indices = [int(i*(1/splits)*len(df)) for i in range(0,splits+1)] # indices where df is to be cut\n",
    "segment_indices = zip(cut_indices[:-1], cut_indices[1:]) # indices for each cut segment\n",
    "valids = [df[begin:end] for begin,end in segment_indices] # subsets each to be used as validation sets\n",
    "\n",
    "# Training sets for each validation set in valids\n",
    "trains = [pd.concat(valids[1:], axis=0)]\n",
    "for n in range(1,splits):\n",
    "    trains += [pd.concat(valids[:n]+valids[n+1:], axis=0)] # (all sets except for set n in range(splits))\n",
    "\n",
    "# Validation and Training Sets to be used\n",
    "validation = valids[0]\n",
    "training = trains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4980875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a mean close to 0.5 for labels indicates a well balanced dataset\n",
    "validation['label'].describe()['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "# VECTORIZE DATASET\n",
    "\n",
    "vocab_size = 5000 # 10000\n",
    "\n",
    "text = training['text'].to_numpy()\n",
    "\n",
    "tok = Tokenizer(num_words=vocab_size, oov_token='<unk>')\n",
    "\n",
    "tok.fit_on_texts(text)\n",
    "\n",
    "tok.word_index['<pad>'] = 0\n",
    "tok.index_word[0] = '<pad>'\n",
    "\n",
    "# pad vectors to maxlength\n",
    "train_text = training['text'].to_numpy()\n",
    "train_seqs = tok.texts_to_sequences(train_text)\n",
    "maxlength = max(len(i) for i in train_seqs)\n",
    "train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, maxlen=maxlength, padding='post')\n",
    "\n",
    "train_labels = training['label'].to_numpy().flatten()\n",
    "\n",
    "# pad vectors to maxlength (don't calculate automatically, or valid set will be of different size)\n",
    "valid_text = validation['text'].to_numpy()\n",
    "valid_seqs = tok.texts_to_sequences(valid_text)\n",
    "valid_seqs = tf.keras.preprocessing.sequence.pad_sequences(valid_seqs, maxlen=maxlength, padding='post')\n",
    "\n",
    "valid_labels = validation['label'].to_numpy().flatten()\n",
    "\n",
    "# Use lowest possible types to speed up training\n",
    "train_seqs = train_seqs.astype('uint16')\n",
    "valid_seqs = valid_seqs.astype('uint16')\n",
    "train_labels = train_labels.astype('bool')\n",
    "valid_labels = valid_labels.astype('bool') # uint8 or bool?\n",
    "\n",
    "# Convert to TF dataset format\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_seqs,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_seqs,valid_labels))\n",
    "\n",
    "print(maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFUL FUNCTIONS\n",
    "\n",
    "def word2vec(words):\n",
    "    if isinstance(words, str):\n",
    "        \"Takes a string of words and returns a list of corresponding integers\"\n",
    "        seq = tok.texts_to_sequences([words])\n",
    "        return np.array(seq).flatten().tolist()\n",
    "    elif isinstance(words, typing.Iterable):\n",
    "        \"Takes a list of strings and returns a list of sequences (lists of corresponding integers)\"\n",
    "        return tok.texts_to_sequences(words)\n",
    "    else:\n",
    "        raise ValueError(f'Words were of type {type(words)} but should be either a string or list of strings')\n",
    "        \n",
    "\n",
    "def vec2word(vec:typing.Iterable[typing.Any]):\n",
    "    if isinstance(vec[0], numbers.Number):\n",
    "        \"Takes a list of ints and returns a corresponding string\"\n",
    "        return \" \".join([list(tok.word_index.keys())[i-1] for i in vec])\n",
    "    elif isinstance(vec[0], typing.Iterable):\n",
    "        \"Takes an array of sequences (i.e., a 2d array) and returns an array of strings\"\n",
    "        return [vec2word(i) for i in vec]\n",
    "    else:\n",
    "        raise ValueError( f'Input list should contain either ints or lists of ints, not {type(vec[0])}')\n",
    "\n",
    "def vec2word_no_pad(vec:typing.Iterable[typing.Any]):\n",
    "    \"Removes padding and converts vectors of ints to strings\"\n",
    "    if isinstance(vec[0], numbers.Number):\n",
    "        \"Takes a list of ints and returns a corresponding string\"\n",
    "        return \" \".join([list(tok.word_index.keys())[i-1] for i in vec if i != 0])\n",
    "    elif isinstance(vec[0], typing.Iterable):\n",
    "        \"Takes an array of sequences (i.e., a 2d array) and returns an array of strings\"\n",
    "        return [vec2word(i) for i in vec if i != 0]\n",
    "    else:\n",
    "        raise ValueError( f'Input list should contain either ints or lists of ints, not {type(vec[0])}')\n",
    "\n",
    "def show_batch(ds):\n",
    "    \"Takes a tensorflow dataset and returns a batch as a dataframe, with labels shown without padding\"\n",
    "    batch_vecs, batch_targets = next(iter(ds.batch(1)))  # iterate through dataset batches\n",
    "    batch_vecs, batch_targets = np.array(batch_vecs)[0], np.array(batch_targets)[0]  # convert tf batch to np array & reduce dimension by 1\n",
    "    return pd.DataFrame(zip(batch_vecs, [vec2word_no_pad(arr) for arr in batch_vecs], batch_targets), columns=['word_vec','text','target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 64, 96, 128, 160, 192, 224, 256, 288]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[32*i for i in range(1,10)] # Choose a multiple of 32 for embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 5000 594935 2500 2 312\n"
     ]
    }
   ],
   "source": [
    "# TRAINING PARAMETERS\n",
    "\n",
    "# Calculates the max_length, which can be used to store the attention weights\n",
    "maxlength = max(len(i) for i in train_seqs)\n",
    "total_vocab_size = len(tok.word_index) # no need to add +1, word_index includes <pad>\n",
    "batch_size = 512 # 256 # 128 # 64\n",
    "buffer_size = 1000 # 500 # 1000\n",
    "embedding_dim = 64 # 32 # 64 # 128 # 256\n",
    "num_steps = len(train_text) // batch_size\n",
    "epochs = num_steps // buffer_size\n",
    "val_steps = len(valid_seqs) // batch_size // epochs\n",
    "learning_rate = 0.001 * 8\n",
    "\n",
    "print(maxlength, vocab_size, total_vocab_size, num_steps, epochs, val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 118), (None,)), types: (tf.uint16, tf.bool)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHUFFLE AND BATCH\n",
    "\n",
    "train_batch = train_ds.shuffle(buffer_size).batch(batch_size)\n",
    "valid_batch = valid_ds.shuffle(buffer_size).batch(batch_size)\n",
    "train_prefetch = train_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE) # prefetch speeds up training\n",
    "valid_prefetch = valid_batch.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_vec</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[452, 1, 2308, 1, 9, 25, 11, 169, 2986, 19, 11...</td>\n",
       "      <td>says &lt;unk&gt; greatest &lt;unk&gt; is not in never fail...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[186, 16, 52, 1, 160, 278, 3496, 3, 23, 176, 4...</td>\n",
       "      <td>ok so back &lt;unk&gt; 4 doesn't appear to be workin...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[9, 74, 16, 120, 62, 3699, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>is still so happy about federer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[1, 7, 82, 2, 99, 27, 9, 4, 95, 51, 20, 11, 29...</td>\n",
       "      <td>&lt;unk&gt; and thanks i hope this is the last time ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[2608, 27, 98, 18, 625, 14, 1672, 1, 9, 90, 16...</td>\n",
       "      <td>discovered this morning that running on flat &lt;...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>[1, 1846, 161, 2, 63, 515, 3, 1, 126, 8, 33, 6...</td>\n",
       "      <td>&lt;unk&gt; lmfao yes i am talking to &lt;unk&gt; did you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>[439, 296, 12, 197, 1520, 2601, 24, 1360, 2081...</td>\n",
       "      <td>sooo excited for our floor seats at american idol</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>[228, 8, 632, 56, 9, 1, 114, 10, 636, 39, 1, 3...</td>\n",
       "      <td>before you ask what is &lt;unk&gt; why it means quot...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>[1, 2, 42, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>&lt;unk&gt; i do too</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>[1119, 62, 219, 35, 4888, 1, 11, 5, 924, 101, ...</td>\n",
       "      <td>worried about looking like rick &lt;unk&gt; in a dre...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              word_vec  \\\n",
       "0    [452, 1, 2308, 1, 9, 25, 11, 169, 2986, 19, 11...   \n",
       "1    [186, 16, 52, 1, 160, 278, 3496, 3, 23, 176, 4...   \n",
       "2    [9, 74, 16, 120, 62, 3699, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3    [1, 7, 82, 2, 99, 27, 9, 4, 95, 51, 20, 11, 29...   \n",
       "4    [2608, 27, 98, 18, 625, 14, 1672, 1, 9, 90, 16...   \n",
       "..                                                 ...   \n",
       "507  [1, 1846, 161, 2, 63, 515, 3, 1, 126, 8, 33, 6...   \n",
       "508  [439, 296, 12, 197, 1520, 2601, 24, 1360, 2081...   \n",
       "509  [228, 8, 632, 56, 9, 1, 114, 10, 636, 39, 1, 3...   \n",
       "510  [1, 2, 42, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "511  [1119, 62, 219, 35, 4888, 1, 11, 5, 924, 101, ...   \n",
       "\n",
       "                                                  text  target  \n",
       "0    says <unk> greatest <unk> is not in never fail...    True  \n",
       "1    ok so back <unk> 4 doesn't appear to be workin...   False  \n",
       "2                      is still so happy about federer    True  \n",
       "3    <unk> and thanks i hope this is the last time ...    True  \n",
       "4    discovered this morning that running on flat <...    True  \n",
       "..                                                 ...     ...  \n",
       "507  <unk> lmfao yes i am talking to <unk> did you ...   False  \n",
       "508  sooo excited for our floor seats at american idol    True  \n",
       "509  before you ask what is <unk> why it means quot...    True  \n",
       "510                                     <unk> i do too   False  \n",
       "511  worried about looking like rick <unk> in a dre...   False  \n",
       "\n",
       "[512 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_batch(valid_prefetch) # Note: determining <unk> words is a bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1000 steps, validate for 312 steps\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 46s 46ms/step - loss: 0.4566 - accuracy: 0.7845 - val_loss: 0.4261 - val_accuracy: 0.8028\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.4159 - accuracy: 0.8085 - val_loss: 0.4091 - val_accuracy: 0.8133\n"
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, input_length=maxlength),\n",
    "    layers.Conv1D(filters=16, kernel_size=3, padding='valid'),\n",
    "    layers.MaxPool1D(),\n",
    "#     layers.Dense(32,activation='relu'),\n",
    "    layers.Bidirectional(layers.GRU(embedding_dim//2, return_sequences=True)), # embedding_dim//2\n",
    "    layers.Bidirectional(layers.GRU(embedding_dim//2, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.GRU(embedding_dim//2, return_sequences=False)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', # categorical_crossentropy for multilabel classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_prefetch,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_prefetch, \n",
    "    validation_steps=val_steps,\n",
    "    steps_per_epoch=buffer_size,\n",
    "    callbacks = [] # [tf.keras.callbacks.ReduceLROnPlateau()] # cp_callback not used\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DESIGN\n",
    "\n",
    "# Attention\n",
    "use_attn = False\n",
    "use_additive_attn = False\n",
    "\n",
    "# RNNs\n",
    "use_rnns = True\n",
    "use_bidirectional = True\n",
    "rnn = layers.GRU\n",
    "hdim = embedding_dim//2 if use_bidirectional else embedding_dim  # hidden dimension of rnns\n",
    "num_extra_rnns = 2  # number of additional rnn layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 16)          3088      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 64)          9600      \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 64)          18816     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 64)                18816     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 370,385\n",
      "Trainable params: 370,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype='uint16')\n",
    "\n",
    "\n",
    "# ADD BEGINNING LAYERS\n",
    "\n",
    "if use_attn:\n",
    "    \n",
    "    head = [\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=maxlength),\n",
    "        layers.Conv1D(filters=100, kernel_size=3, padding='same', activation='elu'),\n",
    "    ]\n",
    "    \n",
    "    Q = inputs # query input\n",
    "    V = inputs # value input\n",
    "    \n",
    "    for f in head:\n",
    "        Q = f(Q)\n",
    "        V = f(V)\n",
    "\n",
    "    attn = layers.Attention() if not use_additive_attn else layers.AdditiveAttention()\n",
    "\n",
    "    QV = attn([Q, V])\n",
    "\n",
    "    # if rnns are used, attention layer returns sequence; otherwise, use pooling\n",
    "    if not use_rnns:\n",
    "        Q = layers.GlobalAveragePooling1D()(Q) # [batch_size, filters]\n",
    "        QV = layers.GlobalAveragePooling1D()(QV) # [batch_size, filters]\n",
    "\n",
    "    chain = layers.Concatenate()([Q, QV])\n",
    "    \n",
    "else:\n",
    "    \n",
    "    head = [\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=maxlength),\n",
    "        layers.Conv1D(filters=16, kernel_size=3, padding='valid', activation='elu'),\n",
    "        layers.MaxPool1D()\n",
    "    ]\n",
    "    \n",
    "    chain = inputs\n",
    "    \n",
    "    for f in head:\n",
    "        chain = f(chain)\n",
    "\n",
    "    \n",
    "# ADD MIDDLE LAYERS\n",
    "\n",
    "if use_rnns:\n",
    "    rnns = [rnn(hdim, return_sequences=True)]*(num_extra_rnns) + [rnn(hdim, return_sequences=False)]\n",
    "    rnns = list(map(layers.Bidirectional, rnns)) if use_bidirectional else rnns\n",
    "    \n",
    "    body = rnns\n",
    "    \n",
    "else:\n",
    "    body = [\n",
    "        layers.Dense(32, activation='elu')\n",
    "    ]\n",
    "\n",
    "\n",
    "# ADD FINAL LAYERS\n",
    "\n",
    "body += [\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "]\n",
    "\n",
    "\n",
    "# CONSTRUCT MODEL\n",
    "\n",
    "for f in body:\n",
    "    chain = f(chain)\n",
    "    \n",
    "outputs = chain\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL DESIGN\n",
    "\n",
    "# # Attention\n",
    "# use_attn = True\n",
    "# use_additive_attn = True\n",
    "\n",
    "# # RNNs\n",
    "# use_rnns = True\n",
    "# use_bidirectional = True\n",
    "# rnn = layers.GRU\n",
    "# hdim = embedding_dim//2 if use_bidirectional else embedding_dim  # hidden dimension of rnns\n",
    "# num_extra_rnns = 0  # number of additional rnn layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1000 steps, validate for 312 steps\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 41s 41ms/step - loss: 0.4444 - accuracy: 0.7938 - val_loss: 0.4291 - val_accuracy: 0.8049\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.4153 - accuracy: 0.8112 - val_loss: 0.4072 - val_accuracy: 0.8134\n"
     ]
    }
   ],
   "source": [
    "# COMPILE\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0)\n",
    "\n",
    "\n",
    "# USE THIS TO RELOAD MODEL\n",
    "# model.load_weights(checkpoint_path)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='binary_crossentropy', # categorical_crossentropy for multilabel classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# RUN\n",
    "\n",
    "history = model.fit(\n",
    "    train_prefetch,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_prefetch, \n",
    "    validation_steps=val_steps,\n",
    "    steps_per_epoch=buffer_size,\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau()] # cp_callback not used\n",
    "    )\n",
    "\n",
    "\n",
    "# model.save_weights(checkpoint_path) # save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DESIGN\n",
    "\n",
    "# Attention\n",
    "use_attn = True\n",
    "use_additive_attn = False\n",
    "\n",
    "# RNNs\n",
    "use_rnns = False\n",
    "use_bidirectional = False\n",
    "rnn = layers.GRU\n",
    "hdim = embedding_dim//2 if use_bidirectional else embedding_dim  # hidden dimension of rnns\n",
    "num_extra_rnns = 0  # number of additional rnn layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 64)     320000      input_2[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 100)    19300       embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, None, 100)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 100)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 100)          0           attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           6432        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 345,765\n",
      "Trainable params: 345,765\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype='uint16')\n",
    "\n",
    "\n",
    "# ADD BEGINNING LAYERS\n",
    "\n",
    "if use_attn:\n",
    "    \n",
    "    head = [\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=maxlength),\n",
    "        layers.Conv1D(filters=100, kernel_size=3, padding='same', activation='elu'),\n",
    "    ]\n",
    "    \n",
    "    Q = inputs # query input\n",
    "    V = inputs # value input\n",
    "    \n",
    "    for f in head:\n",
    "        Q = f(Q)\n",
    "        V = f(V)\n",
    "\n",
    "    attn = layers.Attention() if not use_additive_attn else layers.AdditiveAttention()\n",
    "\n",
    "    QV = attn([Q, V])\n",
    "\n",
    "    # if rnns are used, attention layer returns sequence; otherwise, use pooling\n",
    "    if not use_rnns:\n",
    "        Q = layers.GlobalAveragePooling1D()(Q) # [batch_size, filters]\n",
    "        QV = layers.GlobalAveragePooling1D()(QV) # [batch_size, filters]\n",
    "\n",
    "    chain = layers.Concatenate()([Q, QV])\n",
    "    \n",
    "else:\n",
    "    \n",
    "    head = [\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=maxlength),\n",
    "        layers.Conv1D(filters=16, kernel_size=3, padding='valid', activation='elu'),\n",
    "        layers.MaxPool1D()\n",
    "    ]\n",
    "    \n",
    "    chain = inputs\n",
    "    \n",
    "    for f in head:\n",
    "        chain = f(chain)\n",
    "\n",
    "    \n",
    "# ADD MIDDLE LAYERS\n",
    "\n",
    "if use_rnns:\n",
    "    rnns = [rnn(hdim, return_sequences=True)]*(num_extra_rnns) + [rnn(hdim, return_sequences=False)]\n",
    "    rnns = list(map(layers.Bidirectional, rnns)) if use_bidirectional else rnns\n",
    "    \n",
    "    body = rnns\n",
    "    \n",
    "else:\n",
    "    body = [\n",
    "        layers.Dense(32, activation='elu')\n",
    "    ]\n",
    "\n",
    "\n",
    "# ADD FINAL LAYERS\n",
    "\n",
    "body += [\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "]\n",
    "\n",
    "\n",
    "# CONSTRUCT MODEL\n",
    "\n",
    "for f in body:\n",
    "    chain = f(chain)\n",
    "    \n",
    "outputs = chain\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1000 steps, validate for 312 steps\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 0.4747 - accuracy: 0.7761 - val_loss: 0.4340 - val_accuracy: 0.7968\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 0.4397 - accuracy: 0.7963 - val_loss: 0.4248 - val_accuracy: 0.8036\n"
     ]
    }
   ],
   "source": [
    "# COMPILE\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0)\n",
    "\n",
    "\n",
    "# USE THIS TO RELOAD MODEL\n",
    "# model.load_weights(checkpoint_path)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='binary_crossentropy', # categorical_crossentropy for multilabel classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# RUN\n",
    "\n",
    "history = model.fit(\n",
    "    train_prefetch,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_prefetch, \n",
    "    validation_steps=val_steps,\n",
    "    steps_per_epoch=buffer_size,\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau()] # cp_callback not used\n",
    "    )\n",
    "\n",
    "\n",
    "# model.save_weights(checkpoint_path) # save weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前用 GPU 的結果：\n",
    "\n",
    "    81.28% in 1 min 20 s (1 conv, 1 biLSTM)\n",
    "    81.35% in 1 min 40 s (1 GRU, 1 biGRU)\n",
    "    81.46% in 1 min 51 s (1 conv, 2 biGRU)\n",
    "    81.46% in 2 min 21 s (1 conv, 2 GRU, 1biGRU)\n",
    "    80.96% in 2 min 58 s (attention)\n",
    "    81.54% in 3 min 00 s (1 conv, 3 biGRU)\n",
    "    81.51% in 3 min 26 s (2 GRU, 1 biGRU, no conv)\n",
    "    81.58% in 3 min 54 s (1 conv, 4 biGRU)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gilbert/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: training_2/cp.ckpt/assets\n"
     ]
    }
   ],
   "source": [
    "# SAVE\n",
    "\n",
    "checkpoint_path = \"training_2/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "model.save_weights(checkpoint_path) # save weights\n",
    "model.save(checkpoint_path) # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION FUNCTIONS\n",
    "\n",
    "# maxlength = 118\n",
    "\n",
    "def sentiment(num):\n",
    "    \"Converts a float into the corresponding sentiment label\"\n",
    "    if num < 0.40: return 'negative'\n",
    "    if num > 0.65: return 'positive'\n",
    "    return 'neutral'\n",
    "\n",
    "\n",
    "def give_sentiment(sent):\n",
    "    \"Prints given sentences with their predicted sentiments\"\n",
    "    if isinstance(sent, str):\n",
    "        spaces = len(str)\n",
    "        s = word2vec([sent])\n",
    "        s = tf.keras.preprocessing.sequence.pad_sequences(s, maxlen=maxlength, padding='post').astype('uint16')\n",
    "        val = model.predict(s)[0]\n",
    "        res = sentiment(val)\n",
    "        print(\"\\n\")\n",
    "        print(f\"{sent}\", \" \"*(5+spaces-len(sent)), \"|\", \" \"*10, f\" {res} ({val[0]:.2f})\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    elif isinstance(sent, typing.Iterable):\n",
    "        spaces = max([len(i) for i in sent])\n",
    "        s = word2vec(sent)\n",
    "        s = tf.keras.preprocessing.sequence.pad_sequences(s, maxlen=maxlength, padding='post').astype('uint16')\n",
    "        vals = [i for i in model.predict(s)]\n",
    "        res = [sentiment(i[0]) for i in vals]\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        for (sentence, result, val) in zip(sent, res, vals):\n",
    "\n",
    "            print(f\"{sentence}\", \" \"*(spaces-len(sentence)), \"|\", \" \"*4, f\" {result}  ({val[0]:.2f})\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "My iPad is broken                        |       negative  (0.01)\n",
      "This iPad is fixed                       |       positive  (0.71)\n",
      "You're a terrible person                 |       negative  (0.05)\n",
      "We should water the plants               |       neutral  (0.57)\n",
      "I'm thirsty                              |       neutral  (0.41)\n",
      "What time is it?                         |       positive  (0.70)\n",
      "This cake is delicious                   |       positive  (0.99)\n",
      "This neural net is simple yet effective  |       positive  (0.84)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phrases = [\"My iPad is broken\",\n",
    "           \"This iPad is fixed\",\n",
    "           \"You're a terrible person\",\n",
    "           \"We should water the plants\",\n",
    "           \"I'm thirsty\", \n",
    "           \"What time is it?\",\n",
    "           \"This cake is delicious\",\n",
    "           \"This neural net is simple yet effective\"\n",
    "          ]\n",
    "\n",
    "give_sentiment(phrases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
